# 분석 모형 설계

## 1. 분석 절차 수립

### 과대적합 (Overfitting)
- 모델이 지나치게 데이터를 학습하여 매우 복잡해져, 훈련 데이터에는 잘 맞지만 새로운 데이터에 대한 예측 성능이 떨어짐.

### 과소적합 (Underfitting)
- 모델이 너무 간단하여 데이터를 충분히 설명하지 못하고 예측 성능이 떨어짐.

## 2. 분석 환경 구축

### 분석 도구 선정
- **R**: 통계 분석에 특화, 처리 속도는 느리지만 강력한 시각화 기능 제공.
- **Python**: 간결하고 높은 가독성, R보다 빠른 속도, 시각화는 R에 비해 약함.

### 데이터 분할
- **훈련용(Training)**: 모델 학습 (50%)
- **검증용(Validation)**: 모델 튜닝 (30%)
- **평가용(Test)**: 모델 평가 (20%)
- **과대적합/과소적합 방지 및 데이터 불균형 해결** 위해 데이터 분할이 중요.

# 분석 기법 적용

## 1. 분석기법

### 회귀분석 (Regression Analysis)
- 독립변수가 종속변수에 미치는 영향을 파악.
- **잔차**: 실제값과 예측값의 차이.
- **회귀계수 추정법**: 최소제곱법 (잔차 제곱합 최소화).
- **회귀 모형 평가**: R-square (0 ~ 1 사이 값).

#### 선형회귀분석의 가정
1. **선형성**: 종속변수와 독립변수는 선형 관계.
2. **독립성**: 잔차와 독립변수 간 상관 없음.
3. **정상성**: 잔차가 정규분포 특성 가짐.
4. **등분산성**: 잔차의 분산이 고르게 분포.
5. **비상관성**: 잔차들끼리 상관이 없어야 함.

#### 회귀 모형 변수 선택 방법
- **전진선택법**: 변수 하나씩 추가.
- **후진선택법**: 변수 하나씩 제거.
- **단계별 선택법**: 전진+후진, AIC/BIC를 고려한 변수 추가/제거.

### 로지스틱 회귀분석 (Logistic Regression)
- 종속변수가 범주형 데이터일 때 사용.
- **오즈(odds)**: 성공 확률과 실패 확률의 비.
- **로짓(logit) 변환**: 오즈의 자연로그 취함.
- 확률 증가: 독립변수가 n 증가하면 확률이 e의 n승 만큼 증가.

### 의사결정나무 (Decision Tree)
- **분류**: CHAID, CART (지니지수), C4.5/C5.0 (엔트로피지수).
- **회귀**: CHAID, CART.
- **정지규칙**: 분리 기준을 더 이상 찾지 않음.
- **가지치기(pruning)**: 과적합 방지.

### 인공신경망 (Artificial Neural Networks)
- **다중 퍼셉트론(Multi-Layer Perceptron)**: 은닉층 1개 이상 포함.
- **활성화 함수**:
  - **시그모이드(Sigmoid)**: 0 ~ 1 사이.
  - **소프트맥스(Softmax)**: 다범주 분류에서 사용.
  - **하이퍼볼릭 탄젠트(Tanh)**: -1 ~ 1 사이.
  - **ReLU**: 기울기 소실 문제 해결.
  
#### 인공신경망 과적합 방지
- **규제**: 라쏘(L1), 릿지(L2).
- **드롭아웃(Dropout)**: 일부 뉴런 비활성화.
- **조기종료(Early Stopping)**: 과적합 방지.
- **배치정규화(Batch Normalization)**.

#### 인공신경망 학습 방법
- **역전파(Backpropagation)**: 가중치를 수정하여 오차 감소.
- **경사하강법(Gradient Descent)**: 낮은 기울기로 이동, 최소값(global minimum)을 찾아감.

### 서포트벡터머신 (SVM)
- **초평면(Hyperplane)**: 데이터를 구분하는 경계.
- **서포트벡터(Support Vector)**: 초평면에 가까운 샘플.
- **커널 함수**: 저차원 데이터를 고차원으로 변환.

#### SVM 유형
1. **하드마진 분류**: 오류를 전혀 허용하지 않음.
2. **소프트마진 분류**: 오류를 일부 허용.

### 연관분석 (Association Analysis)
- 항목 간 조건-결과 패턴 발견 (장바구니 분석).
- **Apriori 알고리즘**: 연관 규칙 탐색.

#### 연관분석의 지표
1. **지지도(Support)**: 아이템 세트의 빈도.
2. **신뢰도(Confidence)**: 조건이 주어졌을 때 결과의 확률.
3. **향상도(Lift)**: 신뢰도가 우연히 발생할 확률 대비 얼마나 높은지.

### 군집분석 (Clustering)
- **비지도 학습**: 데이터 간 거리나 유사성 기준으로 군집화.

#### 거리측도
1. **연속형 변수**:
   - **유클리디안 거리(Euclidean)**: 차이의 제곱합의 제곱근.
   - **맨하튼 거리(Manhattan)**: 차이의 절대값의 합.
   - **체비셰프 거리(Chebyshev)**: 차이의 절대값 중 최댓값.
   - **표준화 거리**: 유클리디안 거리를 표준편차로 나눔.
   - **마할라노비스 거리**: 상관성 고려.
2. **범주형 변수**:
   - **자카드 유사도(Jaccard Similarity)**.
   - **코사인 유사도(Cosine Similarity)**.

#### 군집화 방법
1. **계층적 군집분석(Hierarchical Clustering)**:
   - 거리 측정: 최단, 최장, 평균 등.
   - **덴드로그램**: 군집 간 계층적 구조 표현.
2. **비계층적 군집분석**:
   - **K평균 군집화(K-Means)**: 군집 개수 지정.
   - **DBSCAN**: 밀도 기반, 군집 개수 불필요, 이상치에 강함.

## 2. 고급 분석 기법

### 다변량 분석 (Multivariate Analysis)
- **차원 축소**: 데이터의 차원을 축소하여 분석.
- **요인분석(Factor Analysis)**:
  - **요인추출**: 주성분분석(PCA), 공통요인분석.
  - **요인회전**: 요인의 해석을 용이하게 하는 방법.

### 시계열 분석 (Time Series Analysis)
- **정상성(Stationarity)**: 평균과 분산이 일정해야 함.
- **차분(Differencing)**: 시점 차이를 통해 정상성 확보.
- **자기상관(Autocorrelation)**: 과거와 현재의 관계.
- **백색잡음(White Noise)**: 오차 항목.

#### 시계열 모형
1. **자기회귀(AR)**: 과거 값으로 미래 예측.
2. **이동평균(MA)**: 과거 오차를 기반으로 예측.
3. **ARIMA**: 자기회귀 + 이동평균 (ARIMA(p, d, q)).

### 베이지안 기법 (Bayesian Methods)
- **베이즈 정리**: 조건부 확률을 통해 업데이트.
- **나이브 베이즈 분류**: 독립 가정과 베이즈 이론을 결합.

### 인공신경망
1. **DNN (Deep Neural Networks)**: 2개 이상의 은닉층을 가진 신경망.
2. **CNN (Convolutional Neural Networks)**: 이미지 처리에 특화.
3. **RNN (Recurrent Neural Networks)**: 순차적 데이터 학습, LSTM/GRU로 장기 의존성 해결.
4. **오토인코더(Autoencoder)**: 입력 데이터를 압축 후 복원하는 신경망.

### 텍스트 마이닝 (Text Mining)
1. **TDM**: 문서에서 단어의 빈도를 행렬로 표현.
2. **TF-IDF**: 단어의 중요도를 평가하는 지표.
3. **Word2Vec**: 단어를 벡터로 변환.

### 트랜스포머 (Transformer)
- **BERT**: 양방향 언어 모델.
- **GPT**: 생성형 언어 모델.

### 앙상블 분석 (Ensemble Methods)
여러 개의 예측 모형을 결합하여 전체적인 성능을 향상시키는 기법. 앙상블 기법은 일반적으로 분산을 줄여서 모델 성능을 개선할 수 있습니다.

#### 앙상블 기법 종류
1. **보팅(Voting)**:
   - 다수결 방식으로 여러 모델의 예측 결과를 결합.
   - **하드 보팅(Hard Voting)**: 다수의 예측이 동일하면 그 클래스를 선택.
   - **소프트 보팅(Soft Voting)**: 각 클래스의 확률을 평균 내어 가장 높은 확률을 선택.

2. **배깅(Bagging, Bootstrap Aggregating)**:
   - 데이터를 복원 추출(중복 샘플 허용)하여 여러 모델을 학습시키고, 그 결과를 결합.
   - 예: **랜덤 포레스트(Random Forest)**, 여러 개의 결정 트리를 결합하여 예측 성능을 향상.
   - **이상치에 강함**: 데이터의 변동에 영향을 덜 받음.

3. **부스팅(Boosting)**:
   - 이전 모델의 오류를 보정하는 방식으로 학습이 순차적으로 이루어짐.
   - **AdaBoost**, **Gradient Boosting**, **XGBoost**, **LightGBM** 등이 유명한 부스팅 기법.
   - **이상치에 민감**: 모델이 순차적으로 오류를 보정하므로 이상치에 민감할 수 있음.
   - 병렬 처리 불가.

4. **랜덤 포레스트(Random Forest)**:
   - 배깅 기법을 활용하여 여러 개의 의사결정트리를 학습하고 예측 결과를 결합.
   - **과적합 방지**: 트리 구조의 복잡성 제어.
   - **이상치에 강함**: 여러 트리를 결합함으로써 이상치의 영향을 줄일 수 있음.

## 3. 비모수 검정 (Non-parametric Tests)
모집단에 대한 분포 가정이 없거나, 관측 자료의 분포가 불확실할 때 사용되는 검정 방법. 데이터의 순위나 차이를 기반으로 분석.

#### 비모수 검정 종류
1. **부호검정(Sign Test)**:
   - 두 관련 집단 간 차이가 있는지 확인.
   - 차이의 부호만을 사용하여 통계 분석.

2. **순위합검정(Wilcoxon Rank-Sum Test)**:
   - 두 집단의 차이를 비교할 때 사용.
   - 평균 차이가 아닌 순위 차이를 분석.

3. **만-휘트니 U검정(Mann-Whitney U Test)**:
   - 두 집단의 분포가 동일한지 여부를 검정.
   - **Wilcoxon Rank-Sum Test**와 비슷하지만 더 널리 사용됨.

4. **크러스컬-월리스 검정(Kruskal-Wallis Test)**:
   - 세 개 이상의 집단 간 차이를 비교.
   - **일원분산분석(ANOVA)**의 비모수 대체 방법.

---